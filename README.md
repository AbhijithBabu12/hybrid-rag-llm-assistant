# ğŸ¤– McLovin RAG Assistant â€” Hybrid AI Chatbot with Document Intelligence

An intelligent hybrid AI assistant that combines **Retrieval-Augmented Generation (RAG)** with a **general Large Language Model (LLM)** to answer questions from uploaded documents or general knowledge.

Built using **Mistral-7B Instruct**, **LangChain**, **FAISS**, and a **Flask web interface** with real-time streaming responses.

---

## ğŸš€ Features

* ğŸ“„ Upload PDFs and chat with your documents
* ğŸ§  Hybrid AI:

  * Document-based answers (RAG)
  * General knowledge fallback (LLM)
* âš¡ Real-time token streaming responses
* ğŸ” Advanced retrieval pipeline:

  * Semantic embeddings (BGE)
  * Multi-Query Retrieval
  * Cross-Encoder Re-ranking
  * Context Compression
* ğŸ’¬ Conversational memory
* ğŸ“± Responsive UI (Desktop + Mobile)
* ğŸ“‚ File management system
* ğŸ¨ Modern chat interface
* ğŸš€ GPU accelerated inference using 4-bit quantization

---

## ğŸ—ï¸ Architecture

User â†’ Flask API â†’ RAG Pipeline â†’ Vector DB â†’ Mistral LLM â†’ Streaming Response

Components:

* **LLM:** Mistral-7B Instruct (4-bit quantized)
* **Embeddings:** BAAI/bge-small-en-v1.5
* **Vector Store:** FAISS
* **Retriever:** MultiQueryRetriever
* **Reranker:** BGE Cross Encoder
* **Backend:** Flask
* **Frontend:** HTML, CSS, JavaScript
* **Deployment:** Google Colab GPU + ngrok

---

## ğŸ“¸ Results

### ğŸ–¥ï¸ Desktop UI

![Desktop Chat](images/laptop_1.png)

![Desktop With Files](images/laptop_2.png)

## ğŸ“± Mobile UI

<p align="center">
  <img src="images/mobiel_1.jpeg" width="250"/>
  <img src="images/mobile_2.jpeg" width="250"/>
</p>


---

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/yourusername/mclovin-rag-assistant.git
cd mclovin-rag-assistant
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Run Server

```bash
python app.py
```

Server will start at:

```
http://localhost:5001
```

---

## ğŸ“‚ Project Structure

```
project/
â”‚â”€â”€ app.py                # Flask backend
â”‚â”€â”€ rag_pipeline.py       # RAG + LLM pipeline
â”‚â”€â”€ templates/
â”‚     â””â”€â”€ index.html      # Frontend UI
â”‚â”€â”€ files/                # Uploaded documents
â”‚â”€â”€ images/               # Screenshots for README
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md
```

---

## ğŸ§  How It Works

1. User uploads PDF documents.
2. Documents are split into chunks.
3. Chunks are embedded into vector space using BGE embeddings.
4. FAISS stores vectors for semantic search.
5. Query pipeline:

   * Multi-query expansion
   * Retriever search
   * Cross-encoder reranking
   * Context compression
6. Mistral LLM generates the final answer.
7. Tokens stream to UI in real-time.

If no relevant documents are found â†’ assistant switches to general knowledge mode.

---

## ğŸš€ Deployment (Colab GPU)

The project can run on **Google Colab GPU**:

1. Start server inside Colab
2. Use **ngrok** to expose public URL
3. Access chatbot from browser or mobile

This allows running large models without local GPU.

---

## ğŸ› ï¸ Technologies Used

* Python
* Flask
* LangChain
* HuggingFace Transformers
* FAISS
* BitsAndBytes (4-bit quantization)
* HTML / CSS / JavaScript
* Google Colab
* ngrok

---

## ğŸ“ˆ Future Improvements

* Persistent vector database
* Authentication & user sessions
* Source citation display
* Docker deployment
* Cloud hosting (AWS / GCP)

---

## ğŸ‘¨â€ğŸ’» Author

Abhijith Babu
Passionate about ML & AI ğŸš€

ğŸ“Œ GitHub: [https://github.com/AbhijithBabu12]

ğŸ“Œ LinkedIn: [https://www.linkedin.com/in/abhijith-babu-856170201/]
